{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Outlook Email Data all in one JSON File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the first 8519 emails to outlook_emails.json\n"
     ]
    }
   ],
   "source": [
    "import win32com.client\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to clean email body\n",
    "def clean_body(body):\n",
    "    # Check if the body contains HTML\n",
    "    if \"<html\" in body.lower():\n",
    "        soup = BeautifulSoup(body, \"html.parser\")\n",
    "        text = soup.get_text(separator=\" \")  # Extract visible text\n",
    "    else:\n",
    "        text = body  # Plain text email\n",
    "\n",
    "    # Remove excessive whitespace, newlines, and tabs\n",
    "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text).strip()\n",
    "\n",
    "    # Remove links (URLs)\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Normalize Unicode to closest English equivalent\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    # Remove non-ASCII characters (keep only English characters)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# Connect to Outlook\n",
    "outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "\n",
    "# Select the Inbox folder\n",
    "inbox = outlook.GetDefaultFolder(6)  # 6 = Inbox\n",
    "\n",
    "# Fetch emails and sort them by received time (most recent first)\n",
    "messages = inbox.Items\n",
    "messages.Sort(\"[ReceivedTime]\", True)  # Sort by newest first\n",
    "\n",
    "# List to store email data\n",
    "email_data = []\n",
    "\n",
    "# Iterate through the first 10 emails\n",
    "for _, message in enumerate(messages):\n",
    "    try:\n",
    "        email_info = {\n",
    "            \"Subject\": clean_body(message.Subject),  # Normalize subject\n",
    "            \"Sender\": clean_body(message.SenderName),  # Normalize sender name\n",
    "            \"Received\": str(message.ReceivedTime),\n",
    "            \"Body\": clean_body(message.Body)  # Clean, normalize, and remove links\n",
    "        }\n",
    "        email_data.append(email_info)\n",
    "    except Exception as e:\n",
    "        print(\"Error processing email:\", e)\n",
    "\n",
    "# Save to JSON file\n",
    "json_filename = \"outlook_emails.json\"\n",
    "with open(json_filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(email_data, json_file, indent=4)\n",
    "\n",
    "print(f\"Saved the first {len(email_data)} emails to {json_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Save Outlook Email Data across various JSON Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully saved 8519 emails to the 'jsonData' folder.\n"
     ]
    }
   ],
   "source": [
    "import win32com.client\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "output_dir = \"jsonData\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to clean email body\n",
    "def clean_body(body):\n",
    "    if not body:\n",
    "        return \"\"  # Handle empty bodies safely\n",
    "\n",
    "    # Check if the body contains HTML\n",
    "    if \"<html\" in body.lower():\n",
    "        soup = BeautifulSoup(body, \"html.parser\")\n",
    "        text = soup.get_text(separator=\" \")  # Extract visible text\n",
    "    else:\n",
    "        text = body  # Plain text email\n",
    "\n",
    "    # Remove excessive whitespace, newlines, and tabs\n",
    "    text = re.sub(r\"[\\r\\n\\t]+\", \" \", text).strip()\n",
    "    \n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Remove links (URLs)\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    # Normalize Unicode to closest English equivalent\n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    # Remove non-ASCII characters (keep only English characters)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "    # Remove `<` symbol\n",
    "    text = text.replace(\"<\", \"\")\n",
    "\n",
    "    return text\n",
    "\n",
    "# Connect to Outlook\n",
    "outlook = win32com.client.Dispatch(\"Outlook.Application\").GetNamespace(\"MAPI\")\n",
    "\n",
    "# Select the Inbox folder\n",
    "inbox = outlook.GetDefaultFolder(6)  # 6 = Inbox\n",
    "\n",
    "# Get ALL emails (without restrictions)\n",
    "messages = inbox.Items\n",
    "messages.IncludeRecurrences = True  \n",
    "\n",
    "# Use GetFirst() and GetNext() to manually iterate through ALL emails\n",
    "email_list = []\n",
    "message = messages.GetFirst()\n",
    "while message:\n",
    "    email_list.append(message)\n",
    "    message = messages.GetNext()  # Move to the next email\n",
    "\n",
    "# Sort manually by received time (newest first)\n",
    "email_list.sort(key=lambda m: m.ReceivedTime, reverse=True)\n",
    "\n",
    "# Process emails\n",
    "for i, message in enumerate(email_list):\n",
    "    try:\n",
    "        email_info = {\n",
    "            \"Subject\": clean_body(message.Subject),\n",
    "            \"Sender\": clean_body(message.SenderName),\n",
    "            \"Received\": str(message.ReceivedTime),\n",
    "            \"Body\": clean_body(message.Body)\n",
    "        }\n",
    "\n",
    "        # Save each email as a separate JSON file\n",
    "        json_filename = os.path.join(output_dir, f\"email_{i+1}.json\")\n",
    "        with open(json_filename, \"w\", encoding=\"utf-8\") as json_file:\n",
    "            json.dump(email_info, json_file, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing email {i+1}: {e}\")\n",
    "\n",
    "print(f\"✅ Successfully saved {len(email_list)} emails to the '{output_dir}' folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Embed the Data into 768-dim vectors (Gmail)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kingd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03138437c834e2c8239102da746a46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/366 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All embeddings generated and saved successfully in one file!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=device)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = \"C:\\\\Users\\\\kingd\\\\GitHub\\\\School\\\\Data\\\\gmail_emails.csv\"  # Update the path\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Combine relevant columns into a single text input for embeddings\n",
    "df[\"combined_text\"] = df[\"Subject\"].astype(str) + \" \" + df[\"Sender\"].astype(str) + \" \" + df[\"Date\"].astype(str) + \" \" + df[\"Body\"].astype(str)\n",
    "\n",
    "# Create directory to save files\n",
    "output_dir = \"singleEmbedding/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate embeddings for all emails at once\n",
    "all_embeddings = model.encode(df[\"combined_text\"].tolist(), batch_size=96, show_progress_bar=True)\n",
    "\n",
    "# Save all embeddings as one NumPy file\n",
    "npy_file_path = os.path.join(output_dir, \"Gmail_emails_embeddings.npy\")\n",
    "np.save(npy_file_path, all_embeddings)\n",
    "\n",
    "# Add embeddings to DataFrame and save to one CSV file\n",
    "df[\"embedding\"] = all_embeddings.tolist()\n",
    "csv_file_path = os.path.join(output_dir, \"Gmail_emails_embeddings.csv\")\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(\"All embeddings generated and saved successfully in one file!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>Sender</th>\n",
       "      <th>Date</th>\n",
       "      <th>Body</th>\n",
       "      <th>combined_text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whole Foods gets a union</td>\n",
       "      <td>LinkedIn News</td>\n",
       "      <td>2025-01-28 22:15:36+00:00</td>\n",
       "      <td>---------------------------------------- This ...</td>\n",
       "      <td>Whole Foods gets a union LinkedIn News 2025-01...</td>\n",
       "      <td>[-0.021556325256824493, 0.11548731476068497, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Find Textbook Answers by REAL Experts</td>\n",
       "      <td>\"Numerade\"</td>\n",
       "      <td>2025-01-28 22:24:46+00:00</td>\n",
       "      <td>Numerade's expert educators answer your most d...</td>\n",
       "      <td>Find Textbook Answers by REAL Experts \"Numerad...</td>\n",
       "      <td>[-0.025581959635019302, -0.06178336590528488, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hear that sizzle Citrus Lime Shrimp is BACK</td>\n",
       "      <td>QDOBA Mexican Eats</td>\n",
       "      <td>2025-01-29 15:16:04+00:00</td>\n",
       "      <td>Hear that sizzle? Citrus Lime Shrimp is BACK. ...</td>\n",
       "      <td>Hear that sizzle Citrus Lime Shrimp is BACK QD...</td>\n",
       "      <td>[0.010889058001339436, 0.015130116604268551, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi Nick from LinkedIn Premium</td>\n",
       "      <td>Curtis Coatman</td>\n",
       "      <td>2025-01-29 07:23:30-08:00</td>\n",
       "      <td>LinkedIn Your personal LinkedIn Customer Succe...</td>\n",
       "      <td>Hi Nick from LinkedIn Premium Curtis Coatman 2...</td>\n",
       "      <td>[0.008420374244451523, 0.03241467848420143, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Youd be a great fit for this Data Scientist ro...</td>\n",
       "      <td>Glassdoor Jobs</td>\n",
       "      <td>2025-01-27 06:22:14+00:00</td>\n",
       "      <td>Apply Now!                                    ...</td>\n",
       "      <td>Youd be a great fit for this Data Scientist ro...</td>\n",
       "      <td>[-0.0072630057111382484, 0.08883096277713776, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject              Sender  \\\n",
       "0                           Whole Foods gets a union       LinkedIn News   \n",
       "1              Find Textbook Answers by REAL Experts          \"Numerade\"   \n",
       "2        Hear that sizzle Citrus Lime Shrimp is BACK  QDOBA Mexican Eats   \n",
       "3                      Hi Nick from LinkedIn Premium      Curtis Coatman   \n",
       "4  Youd be a great fit for this Data Scientist ro...      Glassdoor Jobs   \n",
       "\n",
       "                        Date  \\\n",
       "0  2025-01-28 22:15:36+00:00   \n",
       "1  2025-01-28 22:24:46+00:00   \n",
       "2  2025-01-29 15:16:04+00:00   \n",
       "3  2025-01-29 07:23:30-08:00   \n",
       "4  2025-01-27 06:22:14+00:00   \n",
       "\n",
       "                                                Body  \\\n",
       "0  ---------------------------------------- This ...   \n",
       "1  Numerade's expert educators answer your most d...   \n",
       "2  Hear that sizzle? Citrus Lime Shrimp is BACK. ...   \n",
       "3  LinkedIn Your personal LinkedIn Customer Succe...   \n",
       "4  Apply Now!                                    ...   \n",
       "\n",
       "                                       combined_text  \\\n",
       "0  Whole Foods gets a union LinkedIn News 2025-01...   \n",
       "1  Find Textbook Answers by REAL Experts \"Numerad...   \n",
       "2  Hear that sizzle Citrus Lime Shrimp is BACK QD...   \n",
       "3  Hi Nick from LinkedIn Premium Curtis Coatman 2...   \n",
       "4  Youd be a great fit for this Data Scientist ro...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.021556325256824493, 0.11548731476068497, -...  \n",
       "1  [-0.025581959635019302, -0.06178336590528488, ...  \n",
       "2  [0.010889058001339436, 0.015130116604268551, -...  \n",
       "3  [0.008420374244451523, 0.03241467848420143, -0...  \n",
       "4  [-0.0072630057111382484, 0.08883096277713776, ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\Gmail_emails_embeddings.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Zip Embedding Files (Optional)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped singleEmbedding into SingleEmbeddingNick.zip successfully!\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the folder to zip and the output zip file name\n",
    "folder_to_zip = \"singleEmbedding\"\n",
    "output_zip = \"SingleEmbeddingNick.zip\"\n",
    "\n",
    "# Create a zip archive of the folder\n",
    "shutil.make_archive(folder_to_zip, 'zip', folder_to_zip)\n",
    "\n",
    "print(f\"Zipped {folder_to_zip} into {output_zip} successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kingd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kingd\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Similarity between 'Reminder: WOWZA, due to demand we are bringing in another photographer Penn State University 2025-02-14' and 'Heads Up: AMAZING NEWS! Due to high demand, we're bringing in an additional photographer at Penn State University! 2025-02-14': 0.9118\n",
      "Similarity between 'Reminder: WOWZA, due to demand we are bringing in another photographer Penn State University 2025-02-14' and 'Reminder: WOWZA! Due to demand, we are bringing in another photographer—Penn State University, 2025-02-14.': 0.9691\n",
      "Similarity between 'Reminder: WOWZA, due to demand we are bringing in another photographer Penn State University 2025-02-14' and 'Notice: Incredible! Due to high interest, we’re adding a second photographer at Penn State University on 2025-02-14.': 0.8657\n",
      "Similarity between 'Heads Up: AMAZING NEWS! Due to high demand, we're bringing in an additional photographer at Penn State University! 2025-02-14' and 'Reminder: WOWZA! Due to demand, we are bringing in another photographer—Penn State University, 2025-02-14.': 0.9225\n",
      "Similarity between 'Heads Up: AMAZING NEWS! Due to high demand, we're bringing in an additional photographer at Penn State University! 2025-02-14' and 'Notice: Incredible! Due to high interest, we’re adding a second photographer at Penn State University on 2025-02-14.': 0.9304\n",
      "Similarity between 'Reminder: WOWZA! Due to demand, we are bringing in another photographer—Penn State University, 2025-02-14.' and 'Notice: Incredible! Due to high interest, we’re adding a second photographer at Penn State University on 2025-02-14.': 0.8829\n",
      "Cosine similarity computation complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=device)\n",
    "\n",
    "# Function to compute cosine similarity between message embeddings\n",
    "def test_similarity(model, messages):\n",
    "    embeddings = model.encode(messages)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Example messages from the emails to see what the cosine similarity is between similar messages\n",
    "messages = [\n",
    "    \"Reminder: WOWZA, due to demand we are bringing in another photographer Penn State University 2025-02-14\",\n",
    "    \"Heads Up: AMAZING NEWS! Due to high demand, we're bringing in an additional photographer at Penn State University! 2025-02-14\",\n",
    "    \"Reminder: WOWZA! Due to demand, we are bringing in another photographer—Penn State University, 2025-02-14.\",\n",
    "    \"Notice: Incredible! Due to high interest, we’re adding a second photographer at Penn State University on 2025-02-14.\"\n",
    "]\n",
    "\n",
    "# Compute similarity\n",
    "similarity_matrix = test_similarity(model, messages)\n",
    "\n",
    "# Print results\n",
    "for i in range(len(messages)):\n",
    "    for j in range(i + 1, len(messages)):\n",
    "        print(f\"Similarity between '{messages[i]}' and '{messages[j]}': {similarity_matrix[i, j]:.4f}\")\n",
    "\n",
    "print(\"Cosine similarity computation complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Plug into a FAISS Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "# Load CSV File\n",
    "csv_path = r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\Gmail_emails_embeddings.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df[\"embedding\"] = df[\"embedding\"].apply(lambda x: np.array(ast.literal_eval(x), dtype=np.float32))\n",
    "\n",
    "\n",
    "embeddings = np.vstack(df[\"embedding\"].values)\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "metadata = {\n",
    "    i: {\n",
    "        \"Subject\": row[\"Subject\"],\n",
    "        \"Sender\": row[\"Sender\"],\n",
    "        \"Date\": row[\"Date\"],\n",
    "        \"Body\": row[\"Body\"],\n",
    "        \"Combined_Text\": row[\"combined_text\"]\n",
    "    }\n",
    "    for i, row in df.iterrows()\n",
    "}\n",
    "\n",
    "# Save FAISS index\n",
    "faiss_index_path = r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\faiss_index.idx\"\n",
    "faiss.write_index(index, faiss_index_path)\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\faiss_metadata.pkl\"\n",
    "with open(metadata_path, \"wb\") as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"FAISS index and metadata saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Print Metadata for Each FAISS Index ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_items' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Print metadata for each FAISS index ID\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'dict_items' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "metadata_path = r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\faiss_metadata.pkl\"\n",
    "\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Print metadata for each FAISS index ID\n",
    "for i, data in metadata.items():\n",
    "    print(f\"Index {i}: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load Saved FAISS Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save faiss index\n",
    "faiss.write_index(index, r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\faiss_index.idx\")\n",
    "print(\"Index saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import pickle\n",
    "\n",
    "index = faiss.read_index(r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\faiss_index.idx\")\n",
    "print(\"Index loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances: [[244.34048 244.40834 244.44711 244.48041 244.48607]]\n",
      "Indices of nearest neighbors: [[15657 11366 20955  5162 12264]]\n"
     ]
    }
   ],
   "source": [
    "query_vector = np.random.random((1, index.d)).astype('float32')\n",
    "\n",
    "D, I = index.search(query_vector, 5)\n",
    "\n",
    "print(\"Distances:\", D)\n",
    "print(\"Indices of nearest neighbors:\", I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(index.is_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Use LangChain for Retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ FAISS index loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LangChain FAISS vector store is ready!\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Paths to FAISS index and metadata\n",
    "faiss_index_path = r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\faiss_index.idx\"\n",
    "metadata_path = r\"C:\\Users\\kingd\\GitHub\\School\\SingleEmbedding\\faiss_metadata.pkl\"\n",
    "\n",
    "# Load FAISS index\n",
    "index = faiss.read_index(faiss_index_path)\n",
    "print(\"✅ FAISS index loaded successfully!\")\n",
    "\n",
    "# Load metadata\n",
    "with open(metadata_path, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "\n",
    "# Convert metadata into LangChain Documents\n",
    "docstore = InMemoryDocstore({\n",
    "    i: Document(page_content=meta[\"Combined_Text\"], metadata=meta)\n",
    "    for i, meta in metadata.items()\n",
    "})\n",
    "\n",
    "# Create index-to-docstore ID mapping\n",
    "index_to_docstore_id = {i: i for i in range(len(metadata))}\n",
    "\n",
    "# Wrap FAISS index in LangChain\n",
    "vector_store = FAISS(\n",
    "    index=index,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id=index_to_docstore_id,\n",
    "    embedding_function=None  \n",
    ")\n",
    "\n",
    "print(\"✅ LangChain FAISS vector store is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate a random query vector (same dimension as FAISS index)\n",
    "query_vector = np.random.random((index.d,)).astype('float32')\n",
    "\n",
    "# Retrieve top 5 results\n",
    "docs = vector_store.similarity_search_by_vector(query_vector, k=20)\n",
    "\n",
    "# Print results\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"🔹 Result {i+1}:\")\n",
    "    print(f\"  Subject: {doc.metadata['Subject']}\")\n",
    "    print(f\"  Sender: {doc.metadata['Sender']}\")\n",
    "    print(f\"  Date: {doc.metadata['Date']}\")\n",
    "    print(f\"  Body: {doc.metadata['Body']}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔹 Closest Match Metadata:\n",
      "  Subject: 100 Years of Community and Tradition\n",
      "  Sender: \"SMU Undergraduate Admission\"\n",
      "  Date: 2019-03-27 16:05:52-04:00\n",
      "  Body: Get involved, stay active and make memories of our beautiful Hilltop. Join the Mustang Nation Confir...\n"
     ]
    }
   ],
   "source": [
    "if docs:\n",
    "    closest_doc = docs[0]  # Closest vector\n",
    "    print(\"\\n🔹 Closest Match Metadata:\")\n",
    "    print(f\"  Subject: {closest_doc.metadata['Subject']}\")\n",
    "    print(f\"  Sender: {closest_doc.metadata['Sender']}\")\n",
    "    print(f\"  Date: {closest_doc.metadata['Date']}\")\n",
    "    print(f\"  Body: {closest_doc.metadata['Body'][:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
